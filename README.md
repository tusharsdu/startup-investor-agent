# 🧠 LLM-based Question Answering System using RAG

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that enhances a **Large Language Model (LLM)** with up-to-date, context-aware knowledge retrieval. Instead of relying solely on the model’s internal memory, it dynamically retrieves relevant documents from a **vector database** to produce accurate, explainable, and current answers.

---

## 🚀 Features

- **Retrieval-Augmented Generation (RAG)** architecture  
- **Embedding-based search** using OpenAI or SentenceTransformers  
- **FAISS or Chroma vector store** for efficient document retrieval  
- **Context-aware LLM responses** with dynamically injected context  
- **Streamlit UI / API endpoint** for interactive QA  
- **Automatic vector index updates** to prevent outdated answers  
- Support for **PDFs, text files, and web sources**

---

## 🏗️ Architecture Overview

